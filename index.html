<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Haoyu Lu">
  <meta name="description" content="Haoyu Lu's Homepage">
  <meta name="keywords" content="Haoyu Lu,卢浩宇,homepage,主页,PhD,computer vision,Tsinghua,3D reconstruction,image generation,Neural rendering, Digital avatar>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Haoyu Lu (卢浩宇)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:67%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haoyu Lu (卢浩宇)</name>
              </p>
              <p style="text-align:center">
                Email: ruclhy1998[at]163.com &nbsp; &nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=LRxi6-UAAAAJ&hl=en">Google Scholar</a> &nbsp; &nbsp;&nbsp;<a href="https://github.com/RERV">Github</a> &nbsp; &nbsp;&nbsp;
              </p>
              <p>Hi there! I am a Ph.D. Student at Renmin University of China, advised by Prof. <a href="https://gsai.ruc.edu.cn/english/luzhiwu">Zhiwu Lu</a>. I also work closely with Dr. <a href="https://dingmyu.github.io/">Mingyu Ding</a> at UC Berkeley and Prof. <a href="https://bo-zhang.me">Bo Zhang</a> at ZJU. Prior to the Ph.D. study, I received my B.E. degree in Computer Science from Renmin University of China in 2021. My research interests lie in multimodal foundation model and video understanding. 
              </p>
            </td>
            <td style="padding:9% 7% 7% 7%;width:40%;max-width:40%">
              <a href="images/luhaoyu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/luhaoyu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>




      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Technical Reports</heading>
        </td>
      </tr>
    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <tr></tr>
<!--         <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" >
        <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                    <source src='images/dreamcraft3d.mp4'>
        </video>
          </div> -->
<!--         </td> -->
      <td style="padding:20px;width:75%;vertical-align:middle">
          <papertitle>DeepSeek-VL: Towards Real-World Vision-Language Understanding</papertitle>
          <br>
          <strong>Haoyu Lu*</strong>, Wen Liu*, Bo Zhang**, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, Chong Ruan (*Equal Contribution, **Project Lead)
          <br>
          <a href="https://huggingface.co/deepseek-ai">[Hugging Face]</a>
          <a href="https://arxiv.org/abs/2403.05525">[PDF]</a>
          <a href="https://github.com/deepseek-ai/DeepSeek-VL">[Code]</a>
          <a href="https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B">[Demo]</a>
          <br>
          <p> Introducing DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. </p>
      </td>
  </tr>


  <tr></tr>
  <!--         <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
          <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                      <source src='images/dreamcraft3d.mp4'>
          </video>
            </div> -->
  <!--         </td> -->
        <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>WenLan (悟道文澜): Bridging vision and language by large-scale multi-modal pre-training</papertitle>
            <br>
            Yuqi Huo, Manli Zhang, Guangzhen Liu, <strong>Haoyu Lu</strong>, Yizhao Gao, Guoxing Yang, Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, Zongzheng Xi, Yueqian Yang, Anwen Hu, Jinming Zhao, Ruichen Li, Yida Zhao, Liang Zhang, Yuqing Song, Xin Hong, Wanqing Cui, Danyang Hou, Yingyan Li, Junyi Li, Peiyu Liu, Zheng Gong, Chuhao Jin, Yuchong Sun, Shizhe Chen, Zhiwu Lu, Zhicheng Dou, Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song, Ji-Rong Wen
            <br>
            <a href="https://arxiv.org/pdf/2103.06561">[PDF]</a>
            <a href="https://cloud.tencent.com/developer/article/1836143">[机器之心]</a>
            <br>
            <p> Introducing WuDao-WenLan, a large-scale Chinese Vision-Language Pre-Training Model. </p>
        </td>
    </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        

        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images\vdt2.png' style="width:110%;max-width:110%; position: absolute;top: 25%">
              </div>
          </td> 
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>VDT: General-purpose Video Diffusion Transformers via Mask Modeling</papertitle>
              <br>
              <strong>Haoyu Lu</strong>, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, Mingyu Ding
              <br>
              ICLR2024
              <br>
              <a href="https://vdt-2023.github.io/">[Project]</a>
              <a href="https://arxiv.org/pdf/2305.13311">[PDF]</a>
              <a href="https://github.com/RERV/VDT">[Code]</a>
              <a href="https://www.jiqizhixin.com/articles/2024-02-25">[机器之心]</a>
              <br>
              <p>We introduce Video Diffusion Transformer (VDT), which pioneers the use of transformers in diffusion-based video generation.</p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images\uniadapter.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
              </div>
          </td> 
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Uniadapter: Unified parameter-efficient transfer learning for cross-modal modeling</papertitle>
              <br>
              <strong>Haoyu Lu</strong>, Yuqi Huo, Guoxing Yang, Zhiwu Lu, Wei Zhan, Masayoshi Tomizuka, Mingyu Ding
              <br>
              ICLR2024
              <br>
              <a href="https://arxiv.org/pdf/2302.06605">[PDF]</a>
              <a href="https://github.com/RERV/UniAdapter">[Code]</a>
              <br>
              <p>We propose UniAdapter, which unifies unimodal and multimodal adapters for parameter-efficient cross-modal adaptation on pre-trained vision-language models.</p>
          </td>
      </tr>
        
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images\lgdn1.png' style="width:100%;max-width:100%; position: absolute;top: 5%">
              </div>
          </td> 
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>LGDN: Language-Guided Denoising Network for Video-Language Modeling</papertitle>
              <br>
              <strong>Haoyu Lu</strong>, Mingyu Ding, Nanyi Fei, Yuqi Huo, Zhiwu Lu
              <br>
              NeurIPS2022, <strong><font color="#FF0000">Spotlight</font></strong> 
              <br>
              <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/a117a3cd54b7affad04618c77c2fb18b-Abstract-Conference.html">[PDF]</a>
              <br>
              <p>We propose Language-Guided Denoising Network (key-frame selection) for video-language modeling.</p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images\bmu.png' style="width:100%;max-width:100%; position: absolute;top: 15%">
              </div>
          </td> 
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Bmu-moco: Bidirectional momentum update for continual video-language modeling</papertitle>
              <br>
              Yizhao Gao, Nanyi Fei, <strong>Haoyu Lu</strong>, Zhiwu Lu, Hao Jiang, Yijie Li, Zhao Cao
              <br>
              NeurIPS2022, <strong><font color="#FF0000">Spotlight</font></strong> 
              <br>
              <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/8ec61d4084443d29c9e47ac60f9aea31-Paper-Conference.pdf">[PDF]</a>
              <br>
              <p>We propose a cross-modal MoCo-based continual learning algorithm with bidirectional momentum update (BMU).</p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images\wenlan2.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td> 
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Towards artificial general intelligence via a multimodal foundation model</papertitle>
              <br>
              Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jingyuan Wen, <strong>Haoyu Lu</strong>, Ruihua Song, Xin Gao, Tao Xiang, Hao Sun, Ji-Rong Wen
              <br>
              Nature Communications
              <br>
              <a href="https://www.nature.com/articles/s41467-022-30761-2">[PDF]</a>
              <br>
              <p>We develop a foundation model pre-trained with huge multimodal data, which can be quickly adapted for various downstream cognitive tasks.</p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images\cots2.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
              </div>
          </td> 
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval</papertitle>
              <br>
              <strong>Haoyu Lu</strong>, Nanyi Fei, Yuqi Huo, Yizhao Gao, Zhiwu Lu, Ji-Rong Wen
              <br>
              CVPR2022
              <br>
              <a href="https://arxiv.org/pdf/2204.07441">[PDF]</a>
              <br>
              <p>We propose COllaborative Two-Stream vision-language pre-training model (COTS) for cross-modal retrieval by enhancing cross-modal interaction.</p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images\NCP.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
              </div>
          </td> 
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Learning versatile neural architectures by propagating network codes</papertitle>
              <br>
              Mingyu Ding, Yuqi Huo, <strong>Haoyu Lu</strong>, Linjie Yang, Zhe Wang, Zhiwu Lu, Jingdong Wang, Ping Luo
              <br>
              ICLR2022
              <br>
              <a href="https://arxiv.org/abs/2103.13253">[PDF]</a>
              <a href="https://network-propagation.github.io/">[Project]</a>
              <br>
              <p>We explores how to design a single neural network capable of adapting to multiple heterogeneous vision tasks, such as image segmentation, 3D detection, and video recognition.</p>
          </td>
      </tr>


      <tr></tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" >
              <img src='images\compressed.png' style="width:90%;max-width:100%; position: absolute;top: 25%">
          </div>
      </td> 
      <td style="padding:20px;width:75%;vertical-align:middle">
          <papertitle>Compressed video contrastive learning</papertitle>
          <br>
          Yuqi Huo, Mingyu Ding, <strong>Haoyu Lu</strong>, Nanyi Fei, Zhiwu Lu, Ji-Rong Wen, Ping Luo
          <br>
          NeurIPS2021
          <br>
          <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/7647966b7343c29048673252e490f736-Paper.pdf">[PDF]</a>
          <br>
          <p>We propose Motion Vector based Cross Guidance Contrastive learning for video self-supervised learning.</p>
      </td>
  </tr>

  <tr></tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one" >
          <img src='images\ijcai.png' style="width:70%;max-width:100%; position: absolute;top: -5%;right: 20%">
      </div>
  </td> 
  <td style="padding:20px;width:75%;vertical-align:middle">
      <papertitle>Self-supervised video representation learning with constrained spatiotemporal jigsaw</papertitle>
      <br>
      Yuqi Huo, Mingyu Ding, <strong>Haoyu Lu</strong>, Zhiwu Lu, Tao Xiang, Ji-Rong Wen, Ziyuan Huang, Jianwen Jiang, Shiwei Zhang, Mingqian Tang, Songfang Huang, Ping Luo
      <br>
      IJCAI2021
      <br>
      <a href="https://www.ijcai.org/proceedings/2021/0104.pdf">[PDF]</a>
      <br>
      <p>We propose a pretext task for self-supervised video representation learning by exploiting spatiotemporal continuity in videos.</p>
  </td>
</tr>


        <tr></tr>
            <td style="padding:20px;width:0%;vertical-align:middle">
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
		<hr style="margin-top:0px">
                <p>The website template was adapted from <a href="https://mrtornado24.github.io/">Jingxiang Sun</a>.</p>
            </td>
        </tr>

      </td>
    </tr>
  </table>
</body>

</html>
